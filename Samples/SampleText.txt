The fastest computers achieve their speed through
highly parallel designs involving hundreds to thousands
of processors, as well as substantial parallelism
within the processors. The majority use off-the-shelf
microprocessors and memory components to construct
the basic node, then replicate these nodes in large
numbers. The number of processors has increased, the
clock rate has increased, and so has the number of
instructions issued and processed in each clock cycle
(using multiple pipelined function units). The challenge
for designers of SPP systems is to keep these
complex processors fed with data so they can perform
close to their potential; this challenge pervades interconnection
hardware, memory system design, compilation
techniques, and algorithm design.
The interconnection hardware for most SPP systems
continues to be custom designed and proprietary
to its vendors. The nodes are interconnected
through a network of switches containing internal
crossbars and use the same very large-scale integration
technology as processor chips. The specific
topology of the interconnect is less visible to the user
than it was a few years ago, since all packets are
directly routed in hardware and most of the communication
time is associated with the node-to-network
interface. SPP communication technology was
recently transferred into more conventional settings,
including commercial machinery, and a number of
research projects have built high-performance systems
from emerging system-area networks, such as
Myrinet in the Berkeley Networks of Workstations
(NOW) [3], or even commodity local-area networks,
such as fast Ethernet in the Beowulf system (see cesdis.gsfc.nasa.gov/linux/beowulf/).
Memory access has always been a key concern for
fast processors, including vector machines, but it has
been a less critical issue for microprocessors, until
recently. Microprocessors have become much faster,
but memory speeds have not kept pace, so multiple
levels of caches are used to bridge the gap. And
because scientific applications tend to access large
data sets, the difficulty of getting the required data to
the multiple functional units in a continuous stream
has increased. Offsetting these shortcomings, many
current high-performance processors incorporate such
additional features as instruction reordering and data
prefetching. While some of these features put the
onus on the compiler for generating code that takes
advantage of them, in other cases, the processor hardware
does the work. Parallel machines add to the
problem of memory access, because the memory is
physically distributed as needed. This distribution
increases memory access time and makes it variable,
with latency depending on the location of the data.
